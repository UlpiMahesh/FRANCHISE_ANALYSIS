{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5c57864-bd2e-4903-866e-49bdfe05e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import re\n",
    "import csv\n",
    "import requests as r\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from concurrent.futures import ThreadPoolExecutor as te,as_completed \n",
    "from concurrent.futures import ThreadPoolExecutor as te\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Create folder if it doesn't exist in current working directory\n",
    "CACHE_DIR = \"cached_pages\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def sanitize_filename(url):\n",
    "    \"\"\"\n",
    "    Create a readable filename from a URL (e.g., franchisebazar.com_industry_food.html)\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc.replace('.', '_')\n",
    "    path = parsed.path.strip('/').replace('/', '_')\n",
    "    filename = f\"{domain}_{path}.html\"\n",
    "    # Remove illegal characters (Windows safe)\n",
    "    filename = re.sub(r'[<>:\"/\\\\|?*]', '', filename)\n",
    "    return filename\n",
    "\n",
    "def cache_url(url, refresh=False, retries=3, delay=2):\n",
    "    filename = sanitize_filename(url)\n",
    "    filepath = os.path.join(CACHE_DIR, filename)\n",
    "\n",
    "    if os.path.exists(filepath) and not refresh:\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            print(f\"[CACHE] Using cached: {filepath}\")\n",
    "            return file.read()\n",
    "\n",
    "    print(f\"[LIVE] Fetching and caching: {url}\")\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            response = r.get(url, timeout=20)  # ✅ timeout increased to 20s\n",
    "            response.raise_for_status()        # ✅ catches HTTP errors like 403/404\n",
    "            with open(filepath, 'w', encoding='utf-8') as file:\n",
    "                file.write(response.text)\n",
    "            return response.text\n",
    "        except r.exceptions.Timeout:           # ✅ retry on timeout\n",
    "            attempt += 1\n",
    "            print(f\"[TIMEOUT] Attempt {attempt} failed for {url}\")\n",
    "            time.sleep(delay)\n",
    "        except r.exceptions.RequestException as e:  # ✅ handles all other request failures\n",
    "            print(f\"[ERROR] Failed to fetch {url}: {e}\")\n",
    "            break\n",
    "\n",
    "    return \"\"  # ✅ graceful fallback on final failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72752562-92ba-4ffb-87b9-17136d7f6d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url = 'https://www.franchisebazar.com/'\n",
    "response = cache_url(url)\n",
    "\n",
    "soup = bs(response,'html.parser')\n",
    "cards = soup.find(\"ul\", class_=\"franchise-container open\")\n",
    "\n",
    "industry_links = cards.find_all('li')\n",
    "\n",
    "industries = {}\n",
    "for li in industry_links:\n",
    "    a_tag = li.find('a')\n",
    "    if a_tag:\n",
    "        link = a_tag['href']\n",
    "        industry = a_tag.get_text(strip=True)\n",
    "        industries[industry]=url+link\n",
    "print(industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51c16d4a-91d6-49e9-81df-efe887a90ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_industries(name,link):\n",
    "    try:\n",
    "        industry_type=name\n",
    "        url = 'https://www.franchisebazar.com/'\n",
    "        response = cache_url(link)\n",
    "        soup = bs(response,'html.parser')\n",
    "        cards = soup.find('div',class_=\"row karya\")\n",
    "        #if no output from above code then empty dictionary will be returned\n",
    "        if not cards:\n",
    "            return (name,{})\n",
    "            \n",
    "        industry_link = cards.find_all('div',class_='investor-card-wrapper')\n",
    "        \n",
    "        franchise_details = {}\n",
    "        for div in industry_link:\n",
    "            link_tag=div.find('a')\n",
    "            location_tag=div.find('div',class_='col-lg-8 col-xs-8 text-right')\n",
    "            name_tag = div.find('div',class_='main-title')\n",
    "            \n",
    "            if name_tag and link_tag and location_tag:\n",
    "                franchise_name = name_tag.get_text(strip=True)\n",
    "                link = link_tag['href']\n",
    "                city = location_tag.get_text(strip=True)\n",
    "                franchise_details[franchise_name]=[url+link,city]\n",
    "        return industry_type,franchise_details\n",
    "        \n",
    "    except Exception as e:\n",
    "        return (name,{\"error\":str(e)})\n",
    "            \n",
    "industry_franchies = []\n",
    "with te(max_workers=10) as executor:\n",
    "    futures=[executor.submit(get_industries,name,link) for name,link in industries.items()]\n",
    "for future in as_completed(futures):\n",
    "    industry_franchies.append((future.result()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecb56c60-a5fd-44c6-8765-30b711cce3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file=r\"C:\\Users\\91994\\Downloads\\Franchise_analysis\\DATA_EXTRACTION\\industry_initial_details.txt\"\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer=csv.writer(file)\n",
    "    writer.writerow([\"INDUSTRY_NAME\",\"franchise_name\",\"city\",'space','investment','no_of_outlets'])\n",
    "    \n",
    "    for iname,idict in industry_franchies:\n",
    "        with te(max_workers=10) as executor:\n",
    "            futures=[executor.submit(get_industry_details,fname,flist[0],flist[1]) for fname,flist in idict.items()]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            writer.writerow([iname,result[0],result[1],result[2],result[3],result[4]])\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2caaf8de-1370-4403-b1b1-5a4917a8e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_industry_details(name,link,city):\n",
    "    response=cache_url(link)\n",
    "    soup = bs(response,'html.parser')\n",
    "    box = soup.find('div',class_='head-list')\n",
    "    if not box:\n",
    "        return([name,city,'','',''])\n",
    "    industry_details = [name,city]\n",
    "    franchise_details_tag=box.find_all('div',class_='head-item')\n",
    "    for div in franchise_details_tag:\n",
    "        head_tags=div.find_all('div',class_='head-title')\n",
    "        desc_tags=div.find_all('div',class_='head-desc')\n",
    "        \n",
    "        for tag in desc_tags:\n",
    "            industry_details.append(tag.get_text(strip=True))\n",
    "    return industry_details\n",
    "                                    \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
